{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "#!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length = 64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load tokenizer with error handling\n",
    "try:\n",
    "    with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "    print(f\"‚úÖ Loaded tokenizer with {len(itos)} tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "def encode(s): \n",
    "    \"\"\"Encode string to token IDs with bounds checking\"\"\"\n",
    "    try:\n",
    "        return [stoi.get(c, 0) for c in s]  # Use .get() to handle missing chars\n",
    "    except Exception as e:\n",
    "        print(f\"Encoding error for '{s}': {e}\")\n",
    "        return [0]  # Return padding token on error\n",
    "\n",
    "def decode(l): \n",
    "    \"\"\"Decode token IDs to string with bounds checking\"\"\"\n",
    "    try:\n",
    "        # Ensure all tokens are within vocabulary bounds\n",
    "        valid_tokens = [i for i in l if isinstance(i, int) and 0 <= i < len(itos)]\n",
    "        return ''.join([itos[i] for i in valid_tokens])\n",
    "    except Exception as e:\n",
    "        print(f\"Decoding error for {l}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"‚úÖ Configuration and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Loading pretrained NanoGPT model...\")\n",
    "    ckpt = torch.load(\"../sft/gpt.pt\", map_location=device, weights_only=False)\n",
    "    \n",
    "    gptconf = GPTConfig(**ckpt['model_args'])\n",
    "    gpt = GPT(gptconf)\n",
    "    \n",
    "    # Clean state dict\n",
    "    state_dict = ckpt['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    \n",
    "    gpt.load_state_dict(state_dict)\n",
    "    gpt.to(device).train()\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in gpt.parameters()):,}\")\n",
    "    print(f\"Model config: {gptconf}\")\n",
    "    \n",
    "    # Test model with a simple forward pass\n",
    "    test_input = torch.randint(0, len(itos), (1, 10), device=device)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = gpt(test_input, full_seq=True)\n",
    "        print(f\"‚úÖ Model forward pass test successful! Output shape: {logits.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Please check if ../sft/gpt.pt exists and is accessible\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f9384",
   "metadata": {},
   "source": [
    "### Generate 100K Negative Responses with GPT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68002576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"=== ROBUST 100K GENERATION WITH AUTO-RESTART CAPABILITY ===\")\n",
    "\n",
    "# Load all questions\n",
    "try:\n",
    "    with open(\"questions_empty.json\", \"r\") as f:\n",
    "        questions = json.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(questions)} questions from questions_empty.json\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading questions: {e}\")\n",
    "    raise\n",
    "\n",
    "def find_latest_checkpoint():\n",
    "    \"\"\"Find the most recent checkpoint file\"\"\"\n",
    "    checkpoint_files = [f for f in os.listdir('.') if f.startswith('temp_responses_')]\n",
    "    if checkpoint_files:\n",
    "        checkpoint_nums = [int(f.split('_')[2].split('.')[0]) for f in checkpoint_files]\n",
    "        latest_num = max(checkpoint_nums)\n",
    "        latest_file = f\"temp_responses_{latest_num}.json\"\n",
    "        return latest_file, latest_num\n",
    "    return None, 0\n",
    "\n",
    "def load_existing_progress():\n",
    "    \"\"\"Load existing progress with error handling\"\"\"\n",
    "    latest_checkpoint, latest_count = find_latest_checkpoint()\n",
    "    existing_responses = []\n",
    "    \n",
    "    if latest_checkpoint:\n",
    "        try:\n",
    "            with open(latest_checkpoint, \"r\") as f:\n",
    "                existing_responses = json.load(f)\n",
    "            print(f\"üîÑ Resuming from: {latest_checkpoint} ({len(existing_responses)} responses)\")\n",
    "            \n",
    "            # Verify data integrity\n",
    "            if len(existing_responses) != latest_count:\n",
    "                print(f\"‚ö†Ô∏è Data mismatch: file has {len(existing_responses)}, expected {latest_count}\")\n",
    "                # Use actual length\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load {latest_checkpoint}: {e}\")\n",
    "            # Try to find working checkpoint\n",
    "            checkpoint_files = sorted([f for f in os.listdir('.') if f.startswith('temp_responses_')], \n",
    "                                    key=lambda x: int(x.split('_')[2].split('.')[0]), reverse=True)\n",
    "            \n",
    "            for backup_file in checkpoint_files[1:]:  # Skip the corrupted one\n",
    "                try:\n",
    "                    with open(backup_file, \"r\") as f:\n",
    "                        existing_responses = json.load(f)\n",
    "                    print(f\"‚úÖ Loaded backup: {backup_file} ({len(existing_responses)} responses)\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return existing_responses\n",
    "\n",
    "# Configuration for stability\n",
    "num_samples = min(100000, len(questions))\n",
    "existing_responses = load_existing_progress()\n",
    "start_idx = len(existing_responses)\n",
    "remaining_samples = num_samples - start_idx\n",
    "\n",
    "print(f\"üéØ Target: {num_samples:,} samples\")\n",
    "print(f\"üìä Progress: {len(existing_responses):,} completed\")\n",
    "print(f\"üöÄ Remaining: {remaining_samples:,} samples\")\n",
    "\n",
    "def ultra_lightweight_generation(prompt):\n",
    "    try:\n",
    "        gpt.eval()\n",
    "        \n",
    "        # Very short prompt processing\n",
    "        clean_prompt = prompt.strip()[:30]  \n",
    "        prompt_tokens = encode(clean_prompt)\n",
    "        valid_tokens = [t for t in prompt_tokens if isinstance(t, int) and 0 <= t < len(itos)]\n",
    "        \n",
    "        if len(valid_tokens) == 0 or len(valid_tokens) > 20:  \n",
    "            return \"I don't know\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = torch.tensor([valid_tokens], dtype=torch.long, device=device)\n",
    "            \n",
    "            try:\n",
    "                # VERY conservative settings\n",
    "                generated_ids, _ = gpt.generate(\n",
    "                    input_ids, \n",
    "                    max_new_tokens=20,    \n",
    "                    temperature=0.7,     \n",
    "                    top_k=30            \n",
    "                )\n",
    "                \n",
    "                new_tokens = generated_ids[0].tolist()[len(valid_tokens):]\n",
    "                valid_new_tokens = [t for t in new_tokens if isinstance(t, int) and 0 <= t < len(itos)]\n",
    "                \n",
    "                if len(valid_new_tokens) == 0:\n",
    "                    return \"I don't know\"\n",
    "                \n",
    "                new_text = decode(valid_new_tokens).strip()\n",
    "                \n",
    "                # Simple cleanup\n",
    "                new_text = new_text.replace('\\n', ' ').strip()\n",
    "                \n",
    "                # Short length limit\n",
    "                if len(new_text) > 30:\n",
    "                    new_text = new_text[:30].split()\n",
    "                    new_text = ' '.join(new_text[:-1]) if len(new_text) > 1 else new_text[0]\n",
    "                \n",
    "                return new_text if new_text else \"I don't know\"\n",
    "                    \n",
    "            except Exception:\n",
    "                # Immediate cleanup\n",
    "                torch.cuda.empty_cache()\n",
    "                return \"I don't know\"\n",
    "                \n",
    "    except Exception:\n",
    "        return \"I don't know\"\n",
    "\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Aggressive memory management\"\"\"\n",
    "    try:\n",
    "        # Multiple cleanup passes\n",
    "        for _ in range(2):\n",
    "            gc.collect()\n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Cleanup warning: {e}\")\n",
    "\n",
    "def safe_save(data, filename):\n",
    "    \"\"\"Safe file saving with backup\"\"\"\n",
    "    try:\n",
    "        # Save to temporary file first\n",
    "        temp_file = f\"temp_{filename}\"\n",
    "        with open(temp_file, \"w\") as f:\n",
    "            json.dump(data, f, indent=1)\n",
    "        \n",
    "        # Move to final location\n",
    "        import shutil\n",
    "        shutil.move(temp_file, filename)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Save error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main generation loop with crash recovery\n",
    "if remaining_samples <= 0:\n",
    "    print(f\"‚úÖ All {num_samples:,} samples already completed!\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Starting generation from sample {start_idx:,}\")\n",
    "    \n",
    "    # Ultra conservative settings for stability\n",
    "    batch_size = 25          # Small batches\n",
    "    save_frequency = 100     # Frequent saves\n",
    "    cleanup_frequency = 20   # Very frequent cleanup\n",
    "    \n",
    "    print(f\"üîß Settings: batch={batch_size}, save_freq={save_frequency}, cleanup_freq={cleanup_frequency}\")\n",
    "    \n",
    "    negative_responses = existing_responses.copy()\n",
    "    start_time = time.time()\n",
    "    last_save_count = len(negative_responses)\n",
    "    \n",
    "    try:\n",
    "        # Create progress bar\n",
    "        with tqdm(total=remaining_samples, desc=\"Generating\", unit=\"samples\", \n",
    "                 initial=0, position=0, leave=True) as pbar:\n",
    "            \n",
    "            for i in range(start_idx, num_samples, batch_size):\n",
    "                batch_end = min(i + batch_size, num_samples)\n",
    "                batch_questions = questions[i:batch_end]\n",
    "                \n",
    "                batch_results = []\n",
    "                for j, q in enumerate(batch_questions):\n",
    "                    try:\n",
    "                        question = q[\"negative\"].strip()\n",
    "                        \n",
    "                        # Generate with minimal resources\n",
    "                        gpt_response = ultra_lightweight_generation(question)\n",
    "                        \n",
    "                        # Simple formatting\n",
    "                        batch_results.append({\n",
    "                            \"negative\": f\"{question} {gpt_response}.\",\n",
    "                            \"positive\": q[\"positive\"].strip()\n",
    "                        })\n",
    "                        \n",
    "                        # Micro-cleanup every few generations\n",
    "                        if (j + 1) % 10 == 0:\n",
    "                            emergency_cleanup()\n",
    "                        \n",
    "                    except Exception:\n",
    "                        # Silent fallback\n",
    "                        safe_q = question if 'question' in locals() else f\"Q{i+j}\"\n",
    "                        batch_results.append({\n",
    "                            \"negative\": f\"{safe_q} I don't know.\",\n",
    "                            \"positive\": q.get(\"positive\", \"Answer\").strip()\n",
    "                        })\n",
    "                \n",
    "                # Add batch results\n",
    "                negative_responses.extend(batch_results)\n",
    "                current_count = len(negative_responses)\n",
    "                \n",
    "                # Update progress\n",
    "                progress_made = current_count - last_save_count\n",
    "                pbar.update(progress_made)\n",
    "                last_save_count = current_count\n",
    "                \n",
    "                # Show current stats\n",
    "                elapsed = time.time() - start_time\n",
    "                if elapsed > 0:\n",
    "                    rate = (current_count - start_idx) / elapsed\n",
    "                    pbar.set_postfix({\n",
    "                        'count': current_count,\n",
    "                        'rate': f'{rate:.1f}/s'\n",
    "                    })\n",
    "                \n",
    "                # Frequent cleanup\n",
    "                if current_count % cleanup_frequency == 0:\n",
    "                    emergency_cleanup()\n",
    "                \n",
    "                # Frequent checkpointing\n",
    "                if current_count % save_frequency == 0:\n",
    "                    checkpoint_file = f\"temp_responses_{current_count}.json\"\n",
    "                    if safe_save(negative_responses, checkpoint_file):\n",
    "                        print(f\"\\nüíæ Checkpoint: {current_count} samples saved\")\n",
    "                    else:\n",
    "                        print(f\"\\n‚ö†Ô∏è Checkpoint save failed at {current_count}\")\n",
    "                \n",
    "                # Brief pause to prevent overheating\n",
    "                if (current_count - start_idx) % 100 == 0:\n",
    "                    time.sleep(0.05)\n",
    "                \n",
    "                # Progress check\n",
    "                if current_count >= num_samples:\n",
    "                    break\n",
    "        \n",
    "        # Final processing\n",
    "        final_count = len(negative_responses)\n",
    "        elapsed_total = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n‚úÖ Generation completed!\")\n",
    "        print(f\"üìä Final count: {final_count:,} samples\")\n",
    "        print(f\"‚è±Ô∏è Total time: {elapsed_total/60:.1f} minutes\")\n",
    "        \n",
    "        # Save final results\n",
    "        final_output = \"negative_responses_100k_gpt.json\"\n",
    "        print(f\"\\nüíæ Saving final results...\")\n",
    "        \n",
    "        if safe_save(negative_responses, final_output):\n",
    "            print(f\"‚úÖ Final save successful: {final_output}\")\n",
    "        else:\n",
    "            # Emergency save with timestamp\n",
    "            emergency_file = f\"emergency_final_{int(time.time())}.json\"\n",
    "            if safe_save(negative_responses, emergency_file):\n",
    "                print(f\"üíæ Emergency save: {emergency_file}\")\n",
    "            else:\n",
    "                print(f\"‚ùå All saves failed! Data in memory only.\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        current_count = len(negative_responses)\n",
    "        print(f\"\\n‚ö†Ô∏è Interrupted at {current_count:,} samples\")\n",
    "        \n",
    "        # Save progress\n",
    "        interrupt_file = f\"interrupt_save_{current_count}.json\"\n",
    "        if safe_save(negative_responses, interrupt_file):\n",
    "            print(f\"üíæ Progress saved: {interrupt_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        current_count = len(negative_responses)\n",
    "        print(f\"\\n‚ùå Error at {current_count:,} samples: {str(e)[:100]}\")\n",
    "        \n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        emergency_cleanup()\n",
    "        print(\"üßπ Cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend to use the AdamW optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(lines) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples: \n",
    "        # ...\n",
    "        # neg_logprob\n",
    "        # pos_logprob \n",
    "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1 \n",
    "        # ...\n",
    "        ###########################################################\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09027262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # ...\n",
    "        ###########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
