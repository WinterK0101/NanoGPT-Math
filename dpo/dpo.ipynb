{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (1.24.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: eval-type-backport in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (0.2.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (2.39.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (1.24.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: eval-type-backport in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (0.2.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from wandb) (2.39.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\loong\\anaconda3\\envs\\gpt\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.6 GB\n",
      "✅ Loaded tokenizer with 74 tokens\n",
      "✅ Configuration and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length = 64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load tokenizer with error handling\n",
    "try:\n",
    "    with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "    print(f\"✅ Loaded tokenizer with {len(itos)} tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "def encode(s): \n",
    "    \"\"\"Encode string to token IDs with bounds checking\"\"\"\n",
    "    try:\n",
    "        return [stoi.get(c, 0) for c in s]  # Use .get() to handle missing chars\n",
    "    except Exception as e:\n",
    "        print(f\"Encoding error for '{s}': {e}\")\n",
    "        return [0]  # Return padding token on error\n",
    "\n",
    "def decode(l): \n",
    "    \"\"\"Decode token IDs to string with bounds checking\"\"\"\n",
    "    try:\n",
    "        # Ensure all tokens are within vocabulary bounds\n",
    "        valid_tokens = [i for i in l if isinstance(i, int) and 0 <= i < len(itos)]\n",
    "        return ''.join([itos[i] for i in valid_tokens])\n",
    "    except Exception as e:\n",
    "        print(f\"Decoding error for {l}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "print(\"✅ Configuration and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained NanoGPT model...\n",
      "✅ Model loaded successfully!\n",
      "Model parameters: 8,838,852\n",
      "Model config: GPTConfig(block_size=256, vocab_size=74, n_layer=6, n_head=6, n_embd=348, dropout=0.2, bias=False)\n",
      "✅ Model loaded successfully!\n",
      "Model parameters: 8,838,852\n",
      "Model config: GPTConfig(block_size=256, vocab_size=74, n_layer=6, n_head=6, n_embd=348, dropout=0.2, bias=False)\n",
      "✅ Model forward pass test successful! Output shape: torch.Size([1, 10, 74])\n",
      "✅ Model forward pass test successful! Output shape: torch.Size([1, 10, 74])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\NTU\\Y2.1\\SC3000\\assignment1\\NanoGPT-Math\\model.py:52: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Loading pretrained NanoGPT model...\")\n",
    "    ckpt = torch.load(\"../sft/gpt.pt\", map_location=device, weights_only=False)\n",
    "    \n",
    "    gptconf = GPTConfig(**ckpt['model_args'])\n",
    "    gpt = GPT(gptconf)\n",
    "    \n",
    "    # Clean state dict\n",
    "    state_dict = ckpt['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    \n",
    "    gpt.load_state_dict(state_dict)\n",
    "    gpt.to(device).train()\n",
    "    \n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in gpt.parameters()):,}\")\n",
    "    print(f\"Model config: {gptconf}\")\n",
    "    \n",
    "    # Test model with a simple forward pass\n",
    "    test_input = torch.randint(0, len(itos), (1, 10), device=device)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = gpt(test_input, full_seq=True)\n",
    "        print(f\"✅ Model forward pass test successful! Output shape: {logits.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Please check if ../sft/gpt.pt exists and is accessible\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f9384",
   "metadata": {},
   "source": [
    "### Generate 100K Negative Responses with GPT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68002576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATING NEGATIVE RESPONSES USING GPT MODEL (MEMORY OPTIMIZED) ===\n",
      "✅ Loaded 100000 questions from questions_empty.json\n",
      "🧪 TEST MODE: Generating 1,000 samples\n",
      "\n",
      "🚀 Auto-proceeding with test generation of 1,000 samples...\n",
      "⚡ Using memory-optimized approach with frequent cleanup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT Generation:  50%|█████     | 500/1000 [00:31<00:30, 16.19samples/s, rate=15.8/s, ETA=0.5min, mem=CPU]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved checkpoint: temp_responses_500.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT Generation: 100%|██████████| 1000/1000 [01:02<00:00, 16.09samples/s, rate=16.1/s, ETA=0.0min, mem=CPU]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Saved checkpoint: temp_responses_1000.json\n",
      "\n",
      "✅ Generated 1,000 negative responses!\n",
      "⏱️  Total time: 62.17 seconds (1.04 minutes)\n",
      "🚀 Average rate: 16.09 samples/second\n",
      "\n",
      "💾 Saving final results to negative_responses_1000_gpt.json...\n",
      "✅ Saved 1,000 responses to negative_responses_1000_gpt.json\n",
      "\n",
      "📋 Sample GPT-generated responses:\n",
      "\n",
      "1. Question: 56-26=?\n",
      "   Response: 56-26=? Sorry, I don't\n",
      "\n",
      "2. Question: 25+98=?\n",
      "   Response: 25+98=? Sory, I don't\n",
      "\n",
      "3. Question: x*14=98,x=?\n",
      "   Response: x*14=98,x=? Sorry, I don't\n",
      "\n",
      "📊 Success Rate: 998/1000 (99.8%)\n",
      "\n",
      "🎉 SUCCESS: Memory-optimized generation completed!\n",
      "📄 Output: negative_responses_1000_gpt.json\n",
      "\n",
      "🔄 To generate full 100k dataset:\n",
      "1. Set test_mode = False in the code above\n",
      "2. Re-run this cell (will take 2-4 hours)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "print(\"=== GENERATING 100K NEGATIVE RESPONSES USING GPT MODEL ===\")\n",
    "\n",
    "# Load all questions\n",
    "try:\n",
    "    with open(\"questions_empty.json\", \"r\") as f:\n",
    "        questions = json.load(f)\n",
    "    print(f\"✅ Loaded {len(questions)} questions from questions_empty.json\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading questions: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configuration for 100K generation\n",
    "num_samples = min(100000, len(questions))\n",
    "print(f\"🚀 Generating {num_samples:,} negative responses with GPU acceleration\")\n",
    "\n",
    "def generate_model_negative_response(prompt, max_new_tokens=20, temperature=0.9, top_k=100):\n",
    "    \"\"\"Generate a negative response using the pretrained NanoGPT model\"\"\"\n",
    "    try:\n",
    "        gpt.eval()\n",
    "        with torch.no_grad():\n",
    "            # Encode the prompt with error handling\n",
    "            prompt_tokens = encode(prompt[:50])  # Limit prompt length\n",
    "            prompt_tokens = [t for t in prompt_tokens if isinstance(t, int) and 0 <= t < len(itos)]\n",
    "            \n",
    "            if len(prompt_tokens) == 0 or len(prompt_tokens) > 30:\n",
    "                return f\"{prompt} Sorry, I cannot help with this.\"\n",
    "            \n",
    "            input_ids = torch.tensor([prompt_tokens], dtype=torch.long, device=device)\n",
    "            \n",
    "            try:\n",
    "                # Generate response\n",
    "                generated_ids, _ = gpt.generate(input_ids, max_new_tokens=max_new_tokens, \n",
    "                                              temperature=temperature, top_k=top_k)\n",
    "                \n",
    "                # Decode with bounds checking\n",
    "                response_tokens = generated_ids[0].tolist()[len(prompt_tokens):]  # Only new tokens\n",
    "                valid_tokens = [t for t in response_tokens if isinstance(t, int) and 0 <= t < len(itos)]\n",
    "                \n",
    "                if len(valid_tokens) == 0:\n",
    "                    return f\"{prompt} Sorry, I do not know.\"\n",
    "                \n",
    "                new_text = decode(valid_tokens).strip()\n",
    "                \n",
    "                # Ensure it's a refusal response\n",
    "                if len(new_text) < 3 or not any(word in new_text.lower() for word in ['sorry', 'cannot', 'don\\'t', 'unable']):\n",
    "                    return f\"{prompt} Sorry, I do not know.\"\n",
    "                \n",
    "                return f\"{prompt} {new_text}\"\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    return f\"{prompt} Sorry, I cannot help with this.\"\n",
    "                raise e\n",
    "                \n",
    "    except Exception as e:\n",
    "        return f\"{prompt} Sorry, I do not know.\"\n",
    "\n",
    "# Memory management function\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n🚀 Starting generation of {num_samples:,} negative responses...\")\n",
    "start_time = time.time()\n",
    "negative_responses = []\n",
    "\n",
    "# Batch processing configuration\n",
    "batch_size = 100  # GPU optimized batch size\n",
    "save_frequency = 5000  # Save progress every 5000 samples\n",
    "\n",
    "try:\n",
    "    with tqdm(total=num_samples, desc=\"GPT Generation\", unit=\"samples\") as pbar:\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch_questions = questions[i:batch_end]\n",
    "            \n",
    "            # Process each question in the batch\n",
    "            batch_results = []\n",
    "            for j, q in enumerate(batch_questions):\n",
    "                question = q[\"negative\"].strip()[:100]  # Limit question length\n",
    "                \n",
    "                try:\n",
    "                    # Generate negative response\n",
    "                    negative_response = generate_model_negative_response(\n",
    "                        question, max_new_tokens=15, temperature=0.9, top_k=50\n",
    "                    )\n",
    "                    \n",
    "                    batch_results.append({\n",
    "                        \"question\": question,\n",
    "                        \"negative_response\": negative_response,\n",
    "                        \"original_positive\": q[\"positive\"].strip()\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Add fallback response\n",
    "                    batch_results.append({\n",
    "                        \"question\": question,\n",
    "                        \"negative_response\": f\"{question} Sorry, I do not know.\",\n",
    "                        \"original_positive\": q[\"positive\"].strip()\n",
    "                    })\n",
    "            \n",
    "            # Add batch results to main list\n",
    "            negative_responses.extend(batch_results)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(len(batch_results))\n",
    "            \n",
    "            # Calculate and show stats\n",
    "            elapsed = time.time() - start_time\n",
    "            if elapsed > 0:\n",
    "                rate = len(negative_responses) / elapsed\n",
    "                remaining = (num_samples - len(negative_responses)) / rate if rate > 0 else 0\n",
    "                pbar.set_postfix({\n",
    "                    'rate': f'{rate:.1f}/s',\n",
    "                    'ETA': f'{remaining/60:.1f}min',\n",
    "                    'GPU': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'\n",
    "                })\n",
    "            \n",
    "            # Periodic saves to prevent data loss\n",
    "            if len(negative_responses) % save_frequency == 0:\n",
    "                temp_file = f\"temp_responses_{len(negative_responses)}.json\"\n",
    "                with open(temp_file, \"w\") as f:\n",
    "                    json.dump(negative_responses, f, indent=2)\n",
    "                print(f\"\\n💾 Saved checkpoint: {temp_file}\")\n",
    "            \n",
    "            # Clear GPU memory every batch\n",
    "            clear_memory()\n",
    "    \n",
    "    # Final processing\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Generated {len(negative_responses):,} negative responses!\")\n",
    "    print(f\"⏱️  Total time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"🚀 Average rate: {len(negative_responses)/total_time:.2f} samples/second\")\n",
    "    \n",
    "    # Save final results\n",
    "    output_path = f\"negative_responses_100k_gpt.json\"\n",
    "    print(f\"\\n💾 Saving final results to {output_path}...\")\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(negative_responses, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Saved {len(negative_responses):,} responses to {output_path}\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(f\"\\n📋 Sample generated responses:\")\n",
    "    for i in range(min(3, len(negative_responses))):\n",
    "        print(f\"{i+1}. Q: {negative_responses[i]['question']}\")\n",
    "        print(f\"   A: {negative_responses[i]['negative_response']}\")\n",
    "    \n",
    "    # Quality analysis\n",
    "    successful_generations = sum(1 for r in negative_responses if len(r['negative_response'].split()) > 3)\n",
    "    print(f\"\\n📊 Success Rate: {successful_generations}/{len(negative_responses)} ({100*successful_generations/len(negative_responses):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n🎉 SUCCESS: {len(negative_responses):,} negative responses generated!\")\n",
    "    print(f\"📄 Output: {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Generation failed: {e}\")\n",
    "    print(f\"📊 Generated {len(negative_responses):,} samples before error\")\n",
    "    \n",
    "    # Emergency save\n",
    "    if negative_responses:\n",
    "        emergency_save = f\"emergency_save_{len(negative_responses)}.json\"\n",
    "        with open(emergency_save, \"w\") as f:\n",
    "            json.dump(negative_responses, f, indent=2)\n",
    "        print(f\"💾 Emergency save: {emergency_save}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    clear_memory()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b51f7",
   "metadata": {},
   "source": [
    "### Convert to DPO Format with Mathematical Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONVERTING GPT RESPONSES TO DPO FORMAT WITH POSITIVE SOLUTIONS ===\")\n",
    "\n",
    "# Load the GPT-generated negative responses\n",
    "input_file = \"negative_responses_100k_gpt.json\"\n",
    "output_file = \"pos_neg_pairs_100k_final.json\"\n",
    "\n",
    "try:\n",
    "    with open(input_file, \"r\") as f:\n",
    "        gpt_responses = json.load(f)\n",
    "    print(f\"✅ Loaded {len(gpt_responses):,} GPT-generated responses from {input_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ {input_file} not found! Please run the GPT generation cell first.\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n🔄 Converting to DPO format and generating positive responses...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dpo_pairs = []\n",
    "for i, item in enumerate(tqdm(gpt_responses, desc=\"Converting to DPO format\")):\n",
    "    question = item[\"question\"]\n",
    "    gpt_negative = item[\"negative_response\"]\n",
    "    \n",
    "    # Create negative response (use GPT-generated response)\n",
    "    negative_response = gpt_negative\n",
    "    \n",
    "    # Generate positive response with mathematical solution\n",
    "    try:\n",
    "        positive_response = solve_math_question(question)\n",
    "    except Exception as e:\n",
    "        # Fallback if solver fails\n",
    "        positive_response = f\"{question} Let me solve this problem step by step.\"\n",
    "    \n",
    "    # Create DPO pair\n",
    "    dpo_pair = {\n",
    "        \"negative\": negative_response,\n",
    "        \"positive\": positive_response\n",
    "    }\n",
    "    dpo_pairs.append(dpo_pair)\n",
    "\n",
    "convert_time = time.time() - start_time\n",
    "print(f\"\\n✅ Converted {len(dpo_pairs):,} pairs in {convert_time:.2f} seconds\")\n",
    "\n",
    "# Save DPO format\n",
    "print(f\"\\n💾 Saving DPO pairs to {output_file}...\")\n",
    "save_start = time.time()\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(dpo_pairs, f, indent=2)\n",
    "\n",
    "save_time = time.time() - save_start\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"✅ Saved to {output_file}\")\n",
    "print(f\"💾 Save time: {save_time:.2f} seconds\")\n",
    "print(f\"⏱️  Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\n📋 Examples of final DPO pairs:\")\n",
    "for i in range(min(3, len(dpo_pairs))):\n",
    "    print(f\"\\n{i+1}:\")\n",
    "    print(f\"  Negative: {dpo_pairs[i]['negative']}\")\n",
    "    print(f\"  Positive: {dpo_pairs[i]['positive']}\")\n",
    "\n",
    "print(f\"\\n🎉 SUCCESS: {len(dpo_pairs):,} DPO pairs ready for training!\")\n",
    "print(f\"📄 Final dataset: {output_file}\")\n",
    "print(f\"🤖 Ready for DPO training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6be20a",
   "metadata": {},
   "source": [
    "### Mathematical Solver Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING MATH SOLVER ===\n",
      "Testing solver on sample questions:\n",
      "1. 56-26=? The answer is 30 because 56 - 26 equals 30.\n",
      "2. 25+98=? The answer is 123 because 25 + 98 equals 123.\n",
      "3. x*14=98,x=? The answer is 7 because x = 98 ÷ 14 = 7.\n",
      "4. x-9=11,x=? The answer is 20 because x = 11 + 9 = 20.\n",
      "5. 40/10=? The answer is 4 because 40 ÷ 10 equals 4.\n",
      "6. 117/x=9,x=? The answer is 13 because x = 117 ÷ 9 = 13.\n",
      "\n",
      "✅ Math solver ready!\n",
      "Now processing the full 100k dataset...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def solve_math_question(question):\n",
    "    \"\"\"\n",
    "    Solve mathematical questions and generate explanations\n",
    "    \"\"\"\n",
    "    question = question.strip()\n",
    "    \n",
    "    # Basic arithmetic operations: addition, subtraction, multiplication, division\n",
    "    # Pattern: number op number = ?\n",
    "    basic_pattern = r'(\\d+)\\s*([\\+\\-\\*/])\\s*(\\d+)\\s*=\\s*\\?'\n",
    "    match = re.match(basic_pattern, question)\n",
    "    \n",
    "    if match:\n",
    "        num1, op, num2 = int(match.group(1)), match.group(2), int(match.group(3))\n",
    "        \n",
    "        if op == '+':\n",
    "            result = num1 + num2\n",
    "            explanation = f\"The answer is {result} because {num1} + {num2} equals {result}.\"\n",
    "        elif op == '-':\n",
    "            result = num1 - num2\n",
    "            explanation = f\"The answer is {result} because {num1} - {num2} equals {result}.\"\n",
    "        elif op == '*':\n",
    "            result = num1 * num2\n",
    "            explanation = f\"The answer is {result} because {num1} × {num2} equals {result}.\"\n",
    "        elif op == '/':\n",
    "            if num2 != 0:\n",
    "                result = num1 // num2\n",
    "                explanation = f\"The answer is {result} because {num1} ÷ {num2} equals {result}.\"\n",
    "            else:\n",
    "                explanation = \"Division by zero is undefined.\"\n",
    "        \n",
    "        return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Division pattern: number / number = ?\n",
    "    div_pattern = r'(\\d+)/(\\d+)\\s*=\\s*\\?'\n",
    "    match = re.match(div_pattern, question)\n",
    "    if match:\n",
    "        num1, num2 = int(match.group(1)), int(match.group(2))\n",
    "        if num2 != 0:\n",
    "            result = num1 // num2\n",
    "            explanation = f\"The answer is {result} because {num1} ÷ {num2} equals {result}.\"\n",
    "            return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Equation solving patterns\n",
    "    # Pattern: x + number = number, x = ?\n",
    "    add_eq_pattern = r'x\\s*\\+\\s*(\\d+)\\s*=\\s*(\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(add_eq_pattern, question)\n",
    "    if match:\n",
    "        b, c = int(match.group(1)), int(match.group(2))\n",
    "        x = c - b\n",
    "        explanation = f\"The answer is {x} because x = {c} - {b} = {x}.\"\n",
    "        return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Pattern: x - number = number, x = ?\n",
    "    sub_eq_pattern = r'x\\s*-\\s*(\\d+)\\s*=\\s*(-?\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(sub_eq_pattern, question)\n",
    "    if match:\n",
    "        b, c = int(match.group(1)), int(match.group(2))\n",
    "        x = c + b\n",
    "        explanation = f\"The answer is {x} because x = {c} + {b} = {x}.\"\n",
    "        return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Pattern: x * number = number, x = ?\n",
    "    mul_eq_pattern = r'x\\s*\\*\\s*(\\d+)\\s*=\\s*(\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(mul_eq_pattern, question)\n",
    "    if match:\n",
    "        b, c = int(match.group(1)), int(match.group(2))\n",
    "        if b != 0:\n",
    "            x = c // b\n",
    "            explanation = f\"The answer is {x} because x = {c} ÷ {b} = {x}.\"\n",
    "            return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Pattern: number / x = number, x = ?\n",
    "    div_eq_pattern = r'(\\d+)/x\\s*=\\s*(\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(div_eq_pattern, question)\n",
    "    if match:\n",
    "        a, c = int(match.group(1)), int(match.group(2))\n",
    "        if c != 0:\n",
    "            x = a // c\n",
    "            explanation = f\"The answer is {x} because x = {a} ÷ {c} = {x}.\"\n",
    "            return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Pattern: number * x = number, x = ?\n",
    "    mul_eq_pattern2 = r'(\\d+)\\s*\\*\\s*x\\s*=\\s*(\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(mul_eq_pattern2, question)\n",
    "    if match:\n",
    "        a, c = int(match.group(1)), int(match.group(2))\n",
    "        if a != 0:\n",
    "            x = c // a\n",
    "            explanation = f\"The answer is {x} because x = {c} ÷ {a} = {x}.\"\n",
    "            return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Pattern: number + x = number, x = ?\n",
    "    add_eq_pattern2 = r'(\\d+)\\s*\\+\\s*x\\s*=\\s*(\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(add_eq_pattern2, question)\n",
    "    if match:\n",
    "        a, c = int(match.group(1)), int(match.group(2))\n",
    "        x = c - a\n",
    "        explanation = f\"The answer is {x} because x = {c} - {a} = {x}.\"\n",
    "        return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Pattern: number - x = number, x = ?\n",
    "    sub_eq_pattern2 = r'(\\d+)\\s*-\\s*x\\s*=\\s*(\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(sub_eq_pattern2, question)\n",
    "    if match:\n",
    "        a, c = int(match.group(1)), int(match.group(2))\n",
    "        x = a - c\n",
    "        explanation = f\"The answer is {x} because x = {a} - {c} = {x}.\"\n",
    "        return f\"{question} {explanation}\"\n",
    "    \n",
    "    # Pattern: number/x = number, x = ?  \n",
    "    div_eq_pattern2 = r'(\\d+)/x\\s*=\\s*(\\d+)\\s*,\\s*x\\s*=\\s*\\?'\n",
    "    match = re.match(div_eq_pattern2, question)\n",
    "    if match:\n",
    "        a, c = int(match.group(1)), int(match.group(2))\n",
    "        if c != 0:\n",
    "            x = a // c\n",
    "            explanation = f\"The answer is {x} because x = {a} ÷ {c} = {x}.\"\n",
    "            return f\"{question} {explanation}\"\n",
    "    \n",
    "    # If no pattern matches, return a generic positive response\n",
    "    explanation = \"Let me solve this step by step and provide the answer.\"\n",
    "    return f\"{question} {explanation}\"\n",
    "\n",
    "# Test the solver function\n",
    "print(\"=== TESTING MATH SOLVER ===\")\n",
    "test_questions = [\n",
    "    \"56-26=?\",\n",
    "    \"25+98=?\", \n",
    "    \"x*14=98,x=?\",\n",
    "    \"x-9=11,x=?\",\n",
    "    \"40/10=?\",\n",
    "    \"117/x=9,x=?\"\n",
    "]\n",
    "\n",
    "print(\"Testing solver on sample questions:\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    result = solve_math_question(q)\n",
    "    print(f\"{i}. {result}\")\n",
    "\n",
    "print(\"\\n✅ Math solver ready!\")\n",
    "print(\"Now processing the full 100k dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca83a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATING POSITIVE RESPONSES FOR 100K QUESTIONS ===\n",
      "✅ Loaded 100,000 pos-neg pairs from pos_neg_pairs_100k.json\n",
      "\n",
      "📋 Current format (first entry):\n",
      "  Negative: 56-26=? Sorry, I do not know!\n",
      "  Positive: 56-26=?\n",
      "\n",
      "🧮 Extracting questions and generating positive responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating positive responses:  30%|███       | 30000/100000 [00:00<00:00, 252015.67questions/s, rate=237658/s, solved=30000/30000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "  Question: 56-26=?\n",
      "  Negative: 56-26=? Sorry, I do not know!\n",
      "  Positive: 56-26=? The answer is 30 because 56 - 26 equals 30.\n",
      "\n",
      "Example 2:\n",
      "  Question: 25+98=?\n",
      "  Negative: 25+98=? Sorry, I do not know!\n",
      "  Positive: 25+98=? The answer is 123 because 25 + 98 equals 123.\n",
      "\n",
      "Example 3:\n",
      "  Question: x*14=98,x=?\n",
      "  Negative: x*14=98,x=? Sorry, I do not know!\n",
      "  Positive: x*14=98,x=? The answer is 7 because x = 98 ÷ 14 = 7.\n",
      "\n",
      "Example 4:\n",
      "  Question: x-9=11,x=?\n",
      "  Negative: x-9=11,x=? Sorry, I do not know!\n",
      "  Positive: x-9=11,x=? The answer is 20 because x = 11 + 9 = 20.\n",
      "\n",
      "Example 5:\n",
      "  Question: x+1=15,x=?\n",
      "  Negative: x+1=15,x=? Sorry, I do not know!\n",
      "  Positive: x+1=15,x=? The answer is 14 because x = 15 - 1 = 14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating positive responses: 100%|██████████| 100000/100000 [00:00<00:00, 279070.31questions/s, rate=272739/s, solved=100000/100000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Generated positive responses for 100,000 questions!\n",
      "🧮 Successfully solved: 100,000/100,000 (100.0%)\n",
      "⏱️  Processing time: 0.37 seconds (0.0 minutes)\n",
      "🚀 Average rate: 271998 questions/second\n",
      "\n",
      "💾 Saving updated data to pos_neg_pairs_100k_with_solutions.json...\n",
      "✅ Saved 100,000 complete pos-neg pairs to pos_neg_pairs_100k_with_solutions.json\n",
      "💾 Save time: 0.29 seconds\n",
      "\n",
      "🔄 Updating original file pos_neg_pairs_100k.json...\n",
      "✅ Updated pos_neg_pairs_100k.json\n",
      "\n",
      "📋 Final examples with solutions:\n",
      "\n",
      "1:\n",
      "  Negative: 56-26=? Sorry, I do not know!\n",
      "  Positive: 56-26=? The answer is 30 because 56 - 26 equals 30.\n",
      "\n",
      "2:\n",
      "  Negative: 25+98=? Sorry, I do not know!\n",
      "  Positive: 25+98=? The answer is 123 because 25 + 98 equals 123.\n",
      "\n",
      "3:\n",
      "  Negative: x*14=98,x=? Sorry, I do not know!\n",
      "  Positive: x*14=98,x=? The answer is 7 because x = 98 ÷ 14 = 7.\n",
      "\n",
      "🎉 SUCCESS: All 100k questions now have proper positive responses!\n",
      "📄 Output files:\n",
      "  - pos_neg_pairs_100k.json (updated)\n",
      "  - pos_neg_pairs_100k_with_solutions.json (backup)\n",
      "🤖 Ready for DPO training!\n"
     ]
    }
   ],
   "source": [
    "# This cell is now handled by the DPO conversion cell above\n",
    "# The mathematical solver is defined in the previous cell and used automatically\n",
    "print(\"ℹ️  Positive response generation is now integrated into the DPO conversion process.\")\n",
    "print(\"✅ Mathematical solutions are generated automatically when converting to DPO format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend to use the AdamW optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(lines) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples: \n",
    "        # ...\n",
    "        # neg_logprob\n",
    "        # pos_logprob \n",
    "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1 \n",
    "        # ...\n",
    "        ###########################################################\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09027262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # ...\n",
    "        ###########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
